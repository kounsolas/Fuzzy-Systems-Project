clc;
clearvars;
close all;

data = readmatrix('epileptic_seizure_data.csv');
data = data(:,2:end);
X = data(:,1:end-1);
Y = data(:,end);
preproc = 1;
fq_tol = 0.01;
[trnData,valData,chkData]=train_test_split(data,preproc,fq_tol);

Xtr = trnData(:,1:end-1);
Ytr = trnData(:,end);
Xval = valData(:,1:end-1);
Yval = valData(:,end);
Xchk = chkData(:,1:end-1);
Ychk = chkData(:,end);

%% class frequencies the same between each set
fprintf("---------INITIAL DATA---------------\n")
frequency = get_frequency(Y);
for i=1:5
fprintf('Class %d frequency: %f\n',i,frequency(i));
end
fprintf("---------TRAINING DATA---------------\n")
frequency = get_frequency(Ytr);
for i=1:5
fprintf('Class %d frequency: %f\n',i,frequency(i));
end
fprintf("---------VALIDATION DATA---------------\n")
frequency = get_frequency(Yval);
for i=1:5
fprintf('Class %d frequency: %f\n',i,frequency(i));
end
fprintf("---------CHECKING DATA DATA---------------\n")
frequency = get_frequency(Ychk);
for i=1:5
fprintf('Class %d frequency: %f\n',i,frequency(i));
end

%% cross validation
num_of_folds = 5;
num_features = [10 20 30 40]';
radius = [0.2 0.3 0.4 0.5]';
c = cvpartition(size(trnData,1), "KFold",num_of_folds);

parameters_val_error = NaN(size(num_features,1),size(radius,1));
fis_rules = NaN(size(num_features,1),size(radius,1));

for i=1:size(num_features,1)
    %feature selection
    k_features = num_features(i);
    fprintf("Number of features: %d\n",k_features);

    for j=1:size(radius,1)
        r_radius = radius(j);
        fprintf("\tRadius: %.2f",r_radius);
        
        fis_rules_fold = NaN(1,num_of_folds);
        fold_val_error = NaN(1,num_of_folds);
        for fold=1:num_of_folds
            fprintf("\t\tFold: %d\n",fold);
            %training and validation fold data
            fold_trData = trnData(c.training(fold)==1,:);
            fold_valData = trnData(c.test(fold)==1,:);

            %feature selection only on fold_trData
            [ranked,~] = relieff(fold_trData(:,1:end-1),fold_trData(:,end),10);
            index_features_selected = ranked(1:k_features);
            cols = [index_features_selected size(trnData,2)]; % add the last column which is the target

            trData_fold_fs = fold_trData(:,cols);
            valData_fold_fs = fold_valData(:,cols);

            %fis with fold data
            options = genfisOptions('SubtractiveClustering','ClusterInfluenceRange',r_radius);
            fis = genfis(trData_fold_fs(:,1:end-1),trData_fold_fs(:,end),options);
            fis_rules_fold(fold) = numel(fis.Rules);

            %train fis
            [trn_fis,trn_error,~,val_fis,val_error] = anfis(trData_fold_fs,fis,[20 0 0.01 0.9 1.1],[],valData_fold_fs);

            %evaluate the hyperparameters with the min mean error
            fold_val_error(fold) = mean(val_error);
        end
        fprintf("Storing fis rules for %d features and %.2f radius\n",k_features,r_radius);
        fis_rules(i,j) = mean(fis_rules_fold);
        fprintf("Storing mean val error for %d features and %.2f radius\n",k_features,r_radius);
        parameters_val_error(i,j) = mean(fold_val_error);
    end
end

%% PLots
text = sprintf("Error Curve | Fis Rules");
figure('Name',text);
scatter(fis_rules,parameters_val_error,'LineWidth',2);
grid on
xlabel("Number of Fis Rules");
ylabel("Error")
title(text);

text = sprintf("Error Curve | Number of Features");
figure('Name',text);
plot(num_features,mean(parameters_val_error,2))
grid on
xlabel("Number of features");
ylabel("Error");
title(text);

text = sprintf('Hyperparameters Scatter plot');
figure('Name',text);
scatter3(num_features,radius,parameters_val_error)

%% Part 3 - Train the TSk model with the optimal hyperparameters

for i=1:size(parameters_val_error,1)
    number_of_features = num_features(i);
    for j=1:size(parameters_val_error,2)
        r=radius(j);
        error = parameters_val_error(i,j);
        fprintf("# features: %d\t\tRadius: %.3f\t\tValidation Error: %.4f\n",number_of_features,r,error)
    end
end

min_error = min(min(parameters_val_error));
[best_row,best_col] = find(parameters_val_error==min_error);
best_number_features = num_features(best_row);
best_radius = radius(best_col);
fprintf("\nOptimal # features: %d\nOptimal Radius: %.3f\n",best_number_features,best_radius);

%feature selection using the optimal hyperparameter
[ranked,~] = relieff(Xtr,Ytr,10);
index_features_selected = ranked(1:best_number_features);
cols = [index_features_selected size(trnData,2)]; %add the last column whichb is the target

trnData_fs = trnData(:,cols);
valData_fs = valData(:,cols);

%fis with fold data
options = genfisOptions('SubtractiveClustering','ClusterInfluenceRange',best_radius);
fis  = genfis(trnData_fs(:,1:end-1),trnData_fs(:,end),options);

%train fis
[trn_fis,trn_error,~,val_fis,val_error]  = anfis(trnData_fs,fis,[10 0 0.01 0.9 1.1],[],valData_fs);

%% Plot the Prediction vs Actual Values
Ypred = evalfis(val_fis,Xchk(:,index_features_selected));
Ypred = max(1, min(5, round(Ypred)));
text = sprintf("|Predictions - Actual Values|");
figure('Name',text);
stem(abs(Ypred-Ychk),'Marker','none');
hold on;
yline(mean(Ychk),LineWidth=2,Color='r');
yline(mean(Ypred),LineWidth=2,Color='g');
grid on;
title(text);
legend(text,'Mean of Actual Values','Mean of Predictions',Location='best');

%% PLot the training vs validation error
text = sprintf('Training Vs Validation Error');
figure('Name',text);
plot(trn_error,LineWidth=2);
grid on;
hold on;
plot(val_error,LineWidth=2);
xlabel('Epochs');
ylabel('Error');
legend('Training Error','Validation Error',location='best');
title(text);

%% Plot some fuzzy
mfs_to_plot = 10;

text = sprintf('Input Memebership functions AFTER training');
figure('Name',text);
plotMFsNew(trn_fis,mfs_to_plot);

text = sprintf('Input Membership functions BEFORE training');
figure('Name',text);
plotMFsNew(fis,mfs_to_plot);

%% Confusion Matrix
[C,labels] = confusionmat(Ychk, Ypred);   % raw counts
Accuracy = sum(diag(C)) / sum(C(:));
fprintf("++++++++++++++++++++++++++++++++++++++++++\n")
fprintf("Accuracy: %f\n",Accuracy)
%Producer's (Recall) and User's (Precision) per class:
Recall = diag(C) ./ sum(C,2);     % per true class (rows)
fprintf("+++++++++++++++++++++++++++++++++++++++++++\n")
fprintf("Recall : %f\n",Recall)
Precision = diag(C) ./ sum(C,1)';    % per predicted class (cols)
fprintf("++++++++++++++++++++++++++++++++++++++++\n")
fprintf("Precision: %f\n",Precision)
kappa = cohenKappa(C);
fprintf("+++++++++++++++++++++++++++++++++++\n")
fprintf("Kappa: %f\n",kappa);

%% Chat - Confusion Matrix
% After: [C,labels] = confusionmat(Ychk, Ypred);

figure; 
confusionchart(C, labels);           % show raw counts with your label order
title('Confusion Matrix');

% (optional) if you prefer per-class recall view:
% confusionchart(C, labels, 'Normalization','row-normalized');










